# My-Magical-Pet #AndroidDevChallenge
An Android App for interacting with a Virtual Pet in AR

# Tell us what your idea is. 

This application will allow any Android User to interact and play with with a Virtual Pet in a magical way using Gestures (Finger), Natural Langauge (voice commands) in Augmented Reality. The app leverages Android AR Core and Google MLKit and Media Pipe open soure project to create this exprience on the edge.

![AR Core Tracking](https://lh6.googleusercontent.com/SUyTSqwCRU_3bHsbHQnic91HuyGJFj_z2B6H4EmWQiaytg4ht5YdOBRzStBYLh8Vi_gXrh6oOSO_gAL-HW_kjPgBRjK0_W15ItBpKNb-sU3KgSXcBvg=w371)

![Finger Tracking](https://github.com/google/mediapipe/blob/master/mediapipe/docs/images/mobile/hand_tracking_3d_android_gpu_small.gif?raw=true)

# Tell us how you plan on bringing it to life. 
Project Plan :

Technologies : 
	
	1. Media Pipe by Google to build Android Model for Hand Tracking that will be used to interact with the Pet : https://github.com/suyashjoshi/mediapipe
	2. Google MLKit : Leveraging TensorFlow Lite for doing on device ML inference 
	2. ARCore (Android SDK) 
	3. Audio & Speech Recogniton : Audio is a big driver in order to create immersive experiences, the visual creates will not only produce sound but users will also be able to speak to them using natural langauge. I will be leveraging this project to train the model and use Speech Recognition https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android

(1) any potential sample code you’ve already written, 
(2) a list of the ways you could use Google’s help, 
(3) as well as the timeline on how you plan on bringing it to life by May 1, 2020. 

Refrences:

1. Google 

# Tell us about you. 
 A great idea is just one part of the equation; we also want to learn a bit more about you. Share with us some of your other projects so we can get an idea of how we can assist you with your project. 


